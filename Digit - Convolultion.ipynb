{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas\n",
    "from ggplot import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in CSV files\n",
    "\n",
    "\n",
    "#load in csv file train data\n",
    "data = pandas.read_csv('train-2.csv', header = 0)\n",
    "\n",
    "#load in csv file test data\n",
    "data_test = pandas.read_csv('test-2.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e4a140897618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data['label']\n",
    "labels=labels.reshape(-1,1)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "label2 = pandas.DataFrame(enc.fit_transform(labels).toarray())\n",
    "data2 = data.drop(['label'], axis=1)\n",
    "\n",
    "\n",
    "data2 = data2.values.reshape(data2.shape[0],28,28,1)\n",
    "data_test = data_test.values.reshape(data_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n",
      "(42000, 28, 28, 1)\n",
      "(42000, 10)\n",
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print data.shape[0]\n",
    "print data2.shape\n",
    "print label2.shape\n",
    "print data_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create training and validation datasets\n",
    "\n",
    "data2= np.array(data2,dtype=np.float32)\n",
    "labels = np.array(label2,dtype = np.float32)\n",
    "data_test = np.array(data_test, dtype = np.float32)\n",
    "train_data, validation_data, train_labels, valid_labels = train_test_split(data2,labels, test_size = 0.15)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function\n",
    "batch_size = 32\n",
    "patch = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "import tensorflow as tf\n",
    "\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, 28,28,1))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, 10))\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(data_test)\n",
    "    \n",
    "    \n",
    "    #Variables:\n",
    "    \n",
    "    weights = {'w1':tf.Variable(tf.truncated_normal([patch, patch,1,depth ], stddev=0.1)),\n",
    "               'w2':tf.Variable(tf.truncated_normal([patch, patch,depth,depth ], stddev=0.1)),\n",
    "               'w3':tf.Variable(tf.truncated_normal([28//4*28//4*depth, num_hidden ], stddev=0.1)),\n",
    "               'out':tf.Variable(tf.truncated_normal([num_hidden,10], stddev=0.1))}\n",
    "    biases = { 'b1': tf.Variable(tf.zeros([depth])),\n",
    "              'b2': tf.Variable(tf.constant(1.0,shape=[depth])),\n",
    "              'b3': tf.Variable(tf.constant(1.0,shape=[num_hidden])),\n",
    "              'out': tf.Variable(tf.constant(1.0,shape=[10]))}\n",
    "        \n",
    "    def model(tf_train_dataset, weights, biases):\n",
    "        conv = tf.nn.conv2d(tf_train_dataset, weights['w1'],[1,2,2,1],padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['b1'])\n",
    "        conv = tf.nn.conv2d(hidden, weights['w2'],[1,2,2,1],padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + biases['b2'])\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden2 = tf.matmul(reshape, weights['w3'])+biases['b3']\n",
    "        hidden2 = tf.nn.relu(hidden2)\n",
    "        logit = tf.matmul(hidden2, weights['out'])+biases['out']\n",
    "        return logit\n",
    "        \n",
    "    #minimize cross entropy\n",
    "    logit = model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit, tf_train_labels))*.01\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #variables for reporting predictions\n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, weights, biases))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, weights, biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 0.746846\n",
      "Training accuracy: 6.2%\n",
      "Validation accuracy: 8.7%\n",
      "Loss at step 500: 0.007188\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 86.3%\n",
      "Loss at step 1000: 0.004217\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1500: 0.004167\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 2000: 0.001872\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 2500: 0.001948\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 3000: 0.005251\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 3500: 0.004078\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 4000: 0.001605\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "#definition of accuracy\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions,1)==np.argmax(labels,1))/predictions.shape[0])\n",
    "\n",
    "results = np.zeros([28000,10])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initialize variales\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for i in range(10001):\n",
    "        offset = (batch_size*i)%(train_labels.shape[0] -batch_size)\n",
    "        batch_data = train_data[offset:offset+batch_size,:,:,:]\n",
    "        batch_labels = train_labels[offset:offset+batch_size,:]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        if (i % 500 == 0):\n",
    "            print('Loss at step %d: %f' % (i, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    results = test_prediction.eval()\n",
    "\n",
    "        \n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0         1         2         3         4         5  \\\n",
      "0      4.234096e-03  0.000249  0.962885  0.002930  0.000923  0.001854   \n",
      "1      7.336589e-01  0.050019  0.016445  0.004117  0.020152  0.037291   \n",
      "2      1.799937e-04  0.027653  0.001401  0.000226  0.255423  0.000735   \n",
      "3      2.511209e-02  0.006296  0.067748  0.005250  0.172719  0.007763   \n",
      "4      4.170848e-04  0.000042  0.008882  0.977106  0.000039  0.006240   \n",
      "5      9.085230e-05  0.000577  0.009330  0.078797  0.002662  0.003370   \n",
      "6      7.682786e-01  0.032688  0.016517  0.011875  0.007420  0.039936   \n",
      "7      1.523377e-03  0.000275  0.016687  0.860037  0.001518  0.077824   \n",
      "8      7.023046e-01  0.016516  0.088684  0.057157  0.000805  0.018195   \n",
      "9      1.211508e-03  0.000176  0.006137  0.904028  0.000411  0.059580   \n",
      "10     3.264086e-03  0.040435  0.009192  0.051175  0.024699  0.124998   \n",
      "11     3.244230e-07  0.000034  0.022560  0.000506  0.000056  0.000083   \n",
      "12     1.832125e-03  0.007912  0.080185  0.111636  0.015163  0.022315   \n",
      "13     8.617387e-01  0.001000  0.008915  0.002099  0.000629  0.020724   \n",
      "14     6.577985e-05  0.003562  0.000202  0.000390  0.947550  0.000921   \n",
      "15     1.767915e-01  0.036463  0.017421  0.499623  0.004750  0.148728   \n",
      "16     4.701789e-02  0.005145  0.212640  0.175032  0.002595  0.160742   \n",
      "17     5.084969e-04  0.827847  0.002371  0.000191  0.010157  0.001030   \n",
      "18     2.749407e-06  0.000142  0.000587  0.003705  0.089857  0.005003   \n",
      "19     9.119934e-01  0.008410  0.002596  0.002491  0.000724  0.021299   \n",
      "20     1.518424e-04  0.007300  0.000470  0.000558  0.357751  0.001493   \n",
      "21     6.419663e-04  0.902742  0.000220  0.000773  0.031526  0.000754   \n",
      "22     1.387503e-02  0.618164  0.009233  0.042591  0.014774  0.016309   \n",
      "23     3.622792e-02  0.003220  0.018174  0.057917  0.013301  0.558741   \n",
      "24     2.491074e-06  0.000219  0.002007  0.000078  0.000175  0.000029   \n",
      "25     4.842010e-04  0.073103  0.000761  0.006617  0.611645  0.020919   \n",
      "26     1.677057e-05  0.000104  0.985697  0.000002  0.000029  0.000016   \n",
      "27     1.706933e-04  0.009377  0.001476  0.001010  0.036486  0.000685   \n",
      "28     8.877082e-06  0.019687  0.000445  0.000212  0.007872  0.000241   \n",
      "29     3.890778e-05  0.004050  0.006195  0.000591  0.001246  0.000311   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "27970  3.141369e-03  0.001809  0.019107  0.305067  0.012403  0.411461   \n",
      "27971  2.094277e-01  0.018816  0.039194  0.074394  0.034092  0.022655   \n",
      "27972  3.206102e-02  0.022433  0.003443  0.000954  0.226139  0.001828   \n",
      "27973  2.747800e-02  0.000980  0.109938  0.013031  0.006612  0.042667   \n",
      "27974  6.402361e-01  0.003142  0.037297  0.002687  0.013242  0.073017   \n",
      "27975  7.923034e-04  0.000211  0.021935  0.781349  0.000243  0.129414   \n",
      "27976  5.515463e-02  0.006664  0.002953  0.000482  0.000668  0.003545   \n",
      "27977  7.946476e-01  0.009967  0.074787  0.079990  0.002661  0.003114   \n",
      "27978  1.017987e-03  0.980345  0.000089  0.000138  0.004274  0.000269   \n",
      "27979  3.214358e-06  0.000964  0.000667  0.001419  0.045977  0.002897   \n",
      "27980  5.219694e-03  0.001200  0.022205  0.893855  0.000573  0.060847   \n",
      "27981  1.012319e-01  0.812997  0.018925  0.000080  0.029101  0.000582   \n",
      "27982  5.634960e-02  0.901034  0.000684  0.001539  0.006876  0.001466   \n",
      "27983  4.985338e-01  0.105847  0.065278  0.012231  0.012351  0.040064   \n",
      "27984  9.454293e-04  0.017637  0.005042  0.005104  0.653420  0.017325   \n",
      "27985  2.211104e-04  0.001255  0.004789  0.508883  0.007628  0.068161   \n",
      "27986  4.298742e-03  0.000633  0.784540  0.000638  0.006030  0.022418   \n",
      "27987  1.742754e-03  0.000006  0.900283  0.000371  0.000387  0.008051   \n",
      "27988  1.023280e-03  0.113743  0.007552  0.012429  0.076162  0.007159   \n",
      "27989  2.616585e-02  0.004900  0.002165  0.000171  0.004364  0.004172   \n",
      "27990  1.030848e-08  0.000006  0.002256  0.000009  0.000005  0.000002   \n",
      "27991  1.108657e-01  0.011200  0.049632  0.012990  0.070442  0.051636   \n",
      "27992  5.776641e-03  0.880286  0.009997  0.003578  0.003241  0.001856   \n",
      "27993  2.347842e-03  0.012337  0.014022  0.002197  0.497653  0.021224   \n",
      "27994  4.138007e-04  0.015289  0.008863  0.000314  0.002001  0.000196   \n",
      "27995  9.558797e-05  0.000467  0.004017  0.003448  0.151370  0.006762   \n",
      "27996  4.524128e-05  0.003016  0.014019  0.000088  0.001101  0.000129   \n",
      "27997  5.189785e-03  0.017727  0.009047  0.793580  0.000754  0.064118   \n",
      "27998  1.018072e-04  0.004179  0.000310  0.000268  0.203661  0.000900   \n",
      "27999  4.652311e-04  0.000022  0.986200  0.000166  0.000302  0.005015   \n",
      "\n",
      "                  6         7         8         9  Index  Final  \n",
      "0      1.098082e-03  0.019611  0.005641  0.000577      1      2  \n",
      "1      1.526183e-02  0.002827  0.119104  0.001123      2      0  \n",
      "2      2.727734e-02  0.365107  0.003530  0.318467      3      7  \n",
      "3      4.352791e-01  0.178699  0.007426  0.093708      4      6  \n",
      "4      3.973885e-03  0.000178  0.001419  0.001704      5      3  \n",
      "5      3.532388e-04  0.255443  0.017406  0.631971      6      9  \n",
      "6      2.691975e-03  0.013527  0.106868  0.000199      7      0  \n",
      "7      1.087776e-03  0.003647  0.031196  0.006204      8      3  \n",
      "8      9.582644e-05  0.001828  0.113998  0.000416      9      0  \n",
      "9      2.254222e-05  0.000630  0.022925  0.004880     10      3  \n",
      "10     4.586670e-04  0.056695  0.678884  0.010199     11      8  \n",
      "11     6.648178e-06  0.972977  0.001691  0.002086     12      7  \n",
      "12     2.389813e-04  0.091111  0.568231  0.101377     13      8  \n",
      "13     3.467019e-04  0.000272  0.104236  0.000039     14      0  \n",
      "14     2.239820e-03  0.017560  0.001178  0.026331     15      4  \n",
      "15     8.612872e-03  0.004722  0.100858  0.002030     16      3  \n",
      "16     1.710469e-02  0.000207  0.348743  0.030773     17      8  \n",
      "17     5.341740e-03  0.139629  0.008869  0.004057     18      1  \n",
      "18     3.728683e-04  0.044270  0.004145  0.851916     19      9  \n",
      "19     6.357827e-04  0.000914  0.050919  0.000019     20      0  \n",
      "20     2.473152e-03  0.409818  0.002045  0.217939     21      7  \n",
      "21     7.807776e-04  0.051716  0.009346  0.001501     22      1  \n",
      "22     7.620793e-03  0.190737  0.082011  0.004683     23      1  \n",
      "23     4.792299e-03  0.000926  0.295928  0.010774     24      5  \n",
      "24     8.666472e-06  0.995688  0.001314  0.000478     25      7  \n",
      "25     8.411873e-03  0.039240  0.038242  0.200576     26      4  \n",
      "26     2.432901e-06  0.013229  0.000498  0.000408     27      2  \n",
      "27     1.519069e-03  0.915829  0.004213  0.029233     28      7  \n",
      "28     9.544236e-04  0.915023  0.003308  0.052248     29      7  \n",
      "29     4.989129e-04  0.978541  0.005037  0.003491     30      7  \n",
      "...             ...       ...       ...       ...    ...    ...  \n",
      "27970  3.787455e-03  0.002969  0.220465  0.019791  27971      5  \n",
      "27971  5.372156e-01  0.015705  0.014074  0.034426  27972      6  \n",
      "27972  6.752462e-01  0.019158  0.002747  0.015990  27973      6  \n",
      "27973  1.124633e-03  0.003754  0.785954  0.008461  27974      8  \n",
      "27974  1.083604e-01  0.000091  0.121786  0.000142  27975      0  \n",
      "27975  3.015416e-04  0.001224  0.060183  0.004347  27976      3  \n",
      "27976  9.265273e-01  0.000040  0.003821  0.000145  27977      6  \n",
      "27977  1.167055e-02  0.002690  0.016235  0.004238  27978      0  \n",
      "27978  4.675225e-03  0.003040  0.006028  0.000123  27979      1  \n",
      "27979  4.451078e-04  0.096154  0.002292  0.849182  27980      9  \n",
      "27980  2.094307e-04  0.000308  0.013811  0.001772  27981      3  \n",
      "27981  9.277526e-03  0.014514  0.012782  0.000510  27982      1  \n",
      "27982  7.993669e-03  0.007254  0.016718  0.000086  27983      1  \n",
      "27983  9.744903e-02  0.023637  0.134632  0.009978  27984      0  \n",
      "27984  1.634722e-02  0.024499  0.010776  0.248903  27985      4  \n",
      "27985  1.602625e-04  0.039744  0.252806  0.116353  27986      3  \n",
      "27986  7.500371e-05  0.013848  0.146028  0.021491  27987      2  \n",
      "27987  4.179908e-04  0.004561  0.082690  0.001490  27988      2  \n",
      "27988  7.241063e-03  0.400485  0.022425  0.351780  27989      7  \n",
      "27989  9.517457e-01  0.000376  0.005221  0.000720  27990      6  \n",
      "27990  3.031839e-07  0.997183  0.000166  0.000373  27991      7  \n",
      "27991  6.621707e-01  0.002120  0.024796  0.004148  27992      6  \n",
      "27992  2.360745e-02  0.062160  0.008750  0.000748  27993      1  \n",
      "27993  6.164001e-02  0.116845  0.016475  0.255260  27994      4  \n",
      "27994  1.247715e-03  0.957072  0.007508  0.007095  27995      7  \n",
      "27995  9.435568e-04  0.054609  0.026703  0.751586  27996      9  \n",
      "27996  4.264275e-04  0.966511  0.003573  0.011091  27997      7  \n",
      "27997  6.018095e-04  0.018279  0.086927  0.003777  27998      3  \n",
      "27998  6.741416e-03  0.402376  0.002507  0.378956  27999      7  \n",
      "27999  4.978800e-05  0.000375  0.007235  0.000170  28000      2  \n",
      "\n",
      "[28000 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "results = pandas.DataFrame(results)\n",
    "results['Index']=results.index +1\n",
    "results['Final']=0\n",
    "#results['Max']=0\n",
    "#results.columns = ['00','01','02','03','04','05','06','07','08','09','Index','Final']\n",
    "results['Final']=results[[0,1,2,3,4,5,6,7,8,9]].idxmax(axis = 1)\n",
    "print results\n",
    "\n",
    "#results.loc[(results[0]==),'Final']=0\n",
    "#results.loc[(results[1]==1),'Final']=1\n",
    "#results.loc[(results[2]==1),'Final']=2\n",
    "#results.loc[(results[3]==1),'Final']=3\n",
    "#results.loc[(results[4]==1),'Final']=4\n",
    "#results.loc[(results[5]==1),'Final']=5\n",
    "#results.loc[(results[6]==1),'Final']=6\n",
    "#results.loc[(results[7]==1),'Final']=7\n",
    "#results.loc[(results[8]==1),'Final']=8\n",
    "#results.loc[(results[9]==1),'Final']=9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Index         Final\n",
      "count  28000.000000  28000.000000\n",
      "mean   14000.500000      4.645143\n",
      "std     8083.048105      2.874993\n",
      "min        1.000000      0.000000\n",
      "25%     7000.750000      2.000000\n",
      "50%    14000.500000      5.000000\n",
      "75%    21000.250000      7.000000\n",
      "max    28000.000000      9.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results.columns = ['00','01','02','03','04','05','06','07','08','09','Index','Final']\n",
    "results = results.drop(['00','01','02','03','04','05','06','07','08','09'],axis=1)\n",
    "\n",
    "\n",
    "print results.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"results.csv\", results, delimiter = ',',fmt = '%.1i',comments='',header = \"ImageID,Label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
